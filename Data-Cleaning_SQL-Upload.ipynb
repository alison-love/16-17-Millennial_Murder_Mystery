{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sqlalchemy import create_engine, Integer, String, Float\n",
    "import psycopg2\n",
    "\n",
    "file_paths = {\n",
    "    \"2019\": '2019.xlsx',\n",
    "    \"2020\": '2020.xlsx',\n",
    "    \"2021\": '2021.xlsx',\n",
    "    \"2022\": '2022.xlsx'\n",
    "}\n",
    "\n",
    "# Loading data for each year\n",
    "data_2019 = pd.read_excel(file_paths[\"2019\"])\n",
    "data_2020 = pd.read_excel(file_paths[\"2020\"])\n",
    "data_2021 = pd.read_excel(file_paths[\"2021\"])\n",
    "data_2022 = pd.read_excel(file_paths[\"2022\"])\n",
    "\n",
    "# Add 'Year' column to each DataFrame\n",
    "data_2019['Year'] = 2019\n",
    "data_2020['Year'] = 2020\n",
    "data_2021['Year'] = 2021\n",
    "data_2022['Year'] = 2022\n",
    "\n",
    "# Concatenate all DataFrames\n",
    "all_data = pd.concat([data_2019, data_2020, data_2021, data_2022])\n",
    "\n",
    "all_data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate all DataFrames\n",
    "all_data = pd.concat([data_2019, data_2020, data_2021, data_2022])\n",
    "\n",
    "all_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy Dataframe\n",
    "cleaned_data = all_data.copy()\n",
    "\n",
    "# Assuming 'all_data' is your DataFrame and 'item' is the column you're checking for NaN values\n",
    "cleaned_data = cleaned_data.dropna(subset=['Item'])\n",
    "cleaned_data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy Dataframe\n",
    "all_data_columns_renamed = cleaned_data.copy()\n",
    "\n",
    "# Rename Columns\n",
    "all_data_columns_renamed = all_data_columns_renamed.rename(columns={\n",
    "    'All\\nconsumer\\nunits': 'all_consumer_units',\n",
    "    'Birth year\\nof 1997\\nor later': 'gen_z',\n",
    "    'Birth year\\nfrom 1981\\nto 1996': 'millennials',\n",
    "    'Birth year\\nfrom 1965\\nto 1980': 'gen_x',\n",
    "    'Birth year\\nfrom 1946\\nto 1964': 'baby_boomers',\n",
    "    'Birth year\\nof 1945\\nor earlier': 'silent_generation',\n",
    "    'Year': 'year',\n",
    "    'Item': 'item'\n",
    "    })\n",
    "\n",
    "all_data_columns_renamed.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy Dataframe\n",
    "df_year_reordered = all_data_columns_renamed.copy()\n",
    "\n",
    "# Ensure 'Year' is the first column\n",
    "column_order = ['year'] + [col for col in df_year_reordered.columns if col != 'year']\n",
    "column_order = df_year_reordered[column_order]\n",
    "\n",
    "# Display the reordered and reindexed DataFrame\n",
    "column_order.head(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy Dataframe\n",
    "null_check = column_order.copy()\n",
    "\n",
    "# Define the subset of columns to check for null values (all columns except 'Year')\n",
    "subset_columns = [col for col in null_check.columns if col != 'year']\n",
    "\n",
    "# Drop rows where all specified columns are null\n",
    "row_drop = null_check.dropna(how='all', subset=subset_columns)\n",
    "# Resetting the index\n",
    "row_drop.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Display the cleaned data\n",
    "row_drop.head(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Append Main Category and Subcategory \n",
    "\n",
    "# Copy Dataframe\n",
    "column_init =row_drop.copy()\n",
    "\n",
    "# Initialize columns for 'Main Category' and 'Subcategory' in 'all_data_drop_blanks'\n",
    "column_init['main_category'] = np.nan\n",
    "column_init['subcategory'] = np.nan\n",
    "\n",
    "main_category = None\n",
    "current_subcategory = None\n",
    "for i in range(len(column_init)):\n",
    "    if pd.isnull(column_init.loc[i, 'all_consumer_units']):\n",
    "        if main_category is None or (i+1 < len(column_init) and pd.isnull(column_init.loc[i+1, 'all_consumer_units'])):\n",
    "            # Current row is identified as a main category\n",
    "            main_category = column_init.loc[i, 'item']\n",
    "            current_subcategory = None  # Reset subcategory for a new main category\n",
    "        else:\n",
    "            # Current row is identified as a subcategory\n",
    "            current_subcategory = column_init.loc[i, 'item']\n",
    "    column_init.loc[i, 'main_category'] = main_category\n",
    "    if current_subcategory is not None:\n",
    "        column_init.loc[i, 'subcategory'] = current_subcategory\n",
    "    else:\n",
    "        column_init.loc[i, 'subcategory'] = column_init.loc[i, 'item']  # Use item as subcategory if no subcategory defined\n",
    "\n",
    "# Display the DataFrame\n",
    "column_init.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy Dataframe\n",
    "item_df = column_init.copy()\n",
    "\n",
    "# For rows not considered subcategories, fill 'Subcategory' with 'Item'\n",
    "item_df['subcategory'].fillna(item_df['item'], inplace=True)\n",
    "\n",
    "# Display the DataFrame\n",
    "item_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy Dataframe\n",
    "nans_clean = item_df.copy()\n",
    "\n",
    "# Optionally, to clean up, fill remaining NaNs in 'Main Category'\n",
    "nans_clean['main_category'].fillna(method='ffill', inplace=True)\n",
    "\n",
    "# Display the DataFrame\n",
    "nans_clean.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy Dataframe\n",
    "colon_dropped = nans_clean.copy()\n",
    "\n",
    "\n",
    "# Define a function to remove ':' from non-empty strings\n",
    "def remove_colons(cell_value):\n",
    "    if isinstance(cell_value, str):\n",
    "        return cell_value.replace(':', '')\n",
    "    else:\n",
    "        return cell_value\n",
    "\n",
    "# Apply the function to the entire DataFrame\n",
    "all_data_drop_colons  = colon_dropped.applymap(remove_colons)\n",
    "\n",
    "# Remove rows containing 'b/', '/c', or 'a/' in any column\n",
    "filtered_df = all_data_drop_colons[~all_data_drop_colons.apply(lambda row: row.astype(str).str.contains('b/|/c|a/').any(), axis=1)]\n",
    "\n",
    "# Display the filtered DataFrame\n",
    "filtered_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy Dataframe\n",
    "remaining_nans = filtered_df.copy()\n",
    "\n",
    "# Assuming 'all_data' is your DataFrame and 'item' is the column you're checking for NaN values\n",
    "final_data = remaining_nans.dropna(subset=['all_consumer_units'])\n",
    "\n",
    "# Display the cleaned data\n",
    "final_data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy Dataframe\n",
    "index_reset = final_data.copy()\n",
    "\n",
    "#Index Reset\n",
    "index_reset.reset_index()\n",
    "\n",
    "# rename data frame\n",
    "final_df = index_reset\n",
    "\n",
    "# Display the cleaned data\n",
    "final_data.head(10)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Database connection parameters\n",
    "host = \"192.168.50.231\"  # or your host, e.g., \"127.0.0.1\"\n",
    "#dbname = \"consumer_data\"\n",
    "user = \"postgres\"\n",
    "password = \"postgres\"\n",
    "port = \"5432\"  # default PostgreSQL port"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to your database\n",
    "conn = psycopg2.connect( user=user, password=password, host=host, port=port)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cursor to execute commands\n",
    "cur = conn.cursor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if the table exists and create it if it does not\n",
    "cur.execute(\"\"\"\n",
    "    SELECT EXISTS (\n",
    "        SELECT FROM pg_tables\n",
    "        WHERE schemaname = 'public' AND tablename  = 'consumer_data'\n",
    "    );\n",
    "\"\"\")\n",
    "\n",
    "exists = cur.fetchone()[0]\n",
    "\n",
    "if not exists:\n",
    "    cur.execute(\"\"\"\n",
    "        CREATE TABLE consumer_data (\n",
    "            id SERIAL PRIMARY KEY,\n",
    "            Year INTEGER,\n",
    "            Item TEXT,\n",
    "            All_Consumer_Units FLOAT,\n",
    "            Gen_Z FLOAT,\n",
    "            Millennials FLOAT,\n",
    "            Gen_X FLOAT,\n",
    "            Baby_Boomers FLOAT,\n",
    "            Silent_Generation FLOAT,\n",
    "            Main_Category TEXT,\n",
    "            Subcategory TEXT\n",
    "        );\n",
    "    \"\"\")\n",
    "    print(\"Table 'consumer_data' created.\")\n",
    "else:\n",
    "    print(\"Table 'consumer_data' already exists.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Commit changes\n",
    "conn.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Close cursor and connection\n",
    "cur.close()\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SQLAlchemy connection string\n",
    "conn_str = f'postgresql://{user}:{password}@{host}:{port}/{dbname}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the SQLAlchemy engine\n",
    "engine = create_engine(f'postgresql://{user}:{password}@{host}/{dbname}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert DataFrame column names to lowercase to ensure consistency\n",
    "#df.columns = [col.lower() for col in df.columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explicitly define the dtype mapping; adjust types as necessary\n",
    "dtype_mapping = {\n",
    "    'id': Integer(),\n",
    "    'year': Integer(),\n",
    "    'item': String(),\n",
    "    'all_consumer_units': Float(),\n",
    "    'gen_z': Float(),\n",
    "    'millennials': Float(),\n",
    "    'gen_x': Float(),\n",
    "    'baby_boomers': Float(),\n",
    "    'silent_generation': Float(),\n",
    "    'maincategory': String(),\n",
    "    'subcategory': String()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# copy dataframe\n",
    "double_precision = final_data.copy()\n",
    "\n",
    "# Define a function to handle invalid values in a column\n",
    "def clean_double_precision_value(value, column_name):\n",
    "    if isinstance(value, str) and (value == 'd/' or value == 'c/'):\n",
    "        # Handle 'd/' and 'c/' by returning None\n",
    "        return None\n",
    "    try:\n",
    "        # Try to convert the value to a float (double precision)\n",
    "        return float(value)\n",
    "    except (ValueError, TypeError):\n",
    "        # Handle other invalid values here, if needed\n",
    "        print(f\"Invalid value in column '{column_name}': {value}\")\n",
    "        return None\n",
    "\n",
    "# Columns with double precision values (customize this based on your DataFrame)\n",
    "double_precision_columns = ['gen_z', 'millennials', 'gen_x', 'baby_boomers', 'silent_generation']\n",
    "\n",
    "# Apply the cleaning function to the specified columns\n",
    "for column in double_precision_columns:\n",
    "    double_precision[column] = double_precision.apply(lambda row: clean_double_precision_value(row[column], column), axis=1)\n",
    "\n",
    "\n",
    "# Use the `to_sql` method to insert data, applying the dtype mapping\n",
    "double_precision.to_sql('consumer_data', engine, if_exists='replace', index=False, dtype=dtype_mapping)\n",
    "\n",
    "print(\"Data successfully inserted into the database.\")\n",
    "\n",
    "# Display the cleaned DataFrame\n",
    "double_precision.head(10)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
